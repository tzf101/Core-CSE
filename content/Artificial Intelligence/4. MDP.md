# Why do we need MDP? 
MDP stands for Markov Decision Process. It's a mathematical framework used for modeling decision making in situations where *outcomes are partly random and partly under the control of a decision maker*. MDPs are useful in various AI applications, particularly in reinforcement learning, where an agent learns to make decisions through trial and error.

### Solving MDPs

Solving an MDP typically involves finding an optimal policy $\pi^*$ that maximizes the expected cumulative reward. Several algorithms exist for solving MDPs:

- **Value Iteration:** Iteratively updating the value of each state based on the Bellman equation until convergence.
- **Policy Iteration:** Alternating between policy evaluation (calculating the value of a policy) and policy improvement (updating the policy based on the current value function).
- **Q-Learning:** A model-free reinforcement learning algorithm that learns the value of action-state pairs without needing a model of the environment.
# 
Policy
Policies In previous deterministic search problems, we used to figure out a plan that results in optimal outcome. But since probabilities are involved, we can’t find a one-planfits-all. So instead, we find a optimal policy, defining what to

# Bellman equation

### Expectation Equation
$V^\pi(s) = \sum_{a \in A} \pi(a | s) \sum_{s' \in S} P(s' | s, a) \left[ R(s, a, s') + \gamma V^\pi(s') \right]$

### Optimality Equation
$V (s) = \text{max}_a \sum_{s′} T(s′∣s, a)[R(s, a, s′) + \gamma V (s )$

### Value iteration
$V_{k+1}(s) = \max_{a} \sum_{s'} T(s, a, s') \left[ R(s, a, s') + \gamma V_k(s') \right]$




Let's compute the value iteration step-by-step for the first four iterations. We'll use the Bellman equation to update the value function \(V(s)\) for each state. The equation we're using is:

\[
$V_{k+1}(s) = \max_{a} \sum_{s'} T(s, a, s') \left[ R(s, a, s') + \gamma V_k(s') \right]$

### Initial Setup

Since there is no discount factor (\(\gamma = 1\)), the equation simplifies to:

\[
$V_{k+1}(s) = \max_{a} \sum_{s'} T(s, a, s') \left[ R(s, a, s') + V_k(s') \right]$
\]

We start with \(V_0(s) = 0\) for all states \(s\). This is a common initialization for value iteration.

#### Iteration 1 (\(k = 0\))

We calculate \(V_1(s)\) for each state \(s\):

- **State 0**:
  \[
  $V_1(0) = \max \left( \underbrace{V_0(\text{Stop})}_{=0}, \underbrace{\sum_{s'} T(0, \text{Draw}, s') (R(0, \text{Draw}, s') + V_0(s'))}_{=0} \right) = 0$
  \]

- **State 2**:
  \[
  $V_1(2) = \max \left( \underbrace{V_0(\text{Stop})}_{=2}, \underbrace{\sum_{s'} T(2, \text{Draw}, s') (R(2, \text{Draw}, s') + V_0(s'))}_{=0} \right) = 2$
  \]

- **State 3**:
  \[
  $V_1(3) = \max \left( \underbrace{V_0(\text{Stop})}_{=3}, \underbrace{\sum_{s'} T(3, \text{Draw}, s') (R(3, \text{Draw}, s') + V_0(s'))}_{=0} \right) = 3$
  \]

- **State 4**:
  \[
  $V_1(4) = \max \left( \underbrace{V_0(\text{Stop})}_{=4}, \underbrace{\sum_{s'} T(4, \text{Draw}, s') (R(4, \text{Draw}, s') + V_0(s'))}_{=0} \right) = 4$
  \]

- **State 5**:
  \[
  $V_1(5) = \max \left( \underbrace{V_0(\text{Stop})}_{=5}, \underbrace{\sum_{s'} T(5, \text{Draw}, s') (R(5, \text{Draw}, s') + V_0(s'))}_{=0} \right) = 5$
  \]

- **State Done**:
  \[
  $V_1(\text{Done}) = 0 \quad \text{(no further actions)}$
  \]

#### Iteration 2 (\(k = 1\))

We now calculate \(V_2(s)\) for each state \(s\):

- **State 0**:
  \[
  $V_2(0) = \max \left(0, \frac{1}{3}(0 + V_1(2)) + \frac{1}{3}(0 + V_1(3)) + \frac{1}{3}(0 + V_1(4)) \right) = \max(0, \frac{1}{3}(2 + 3 + 4)) = \max(0, 3) = 3$
  \]

- **State 2**:
  \[
  $V_2(2) = \max \left(2, \frac{1}{3}(0 + V_1(4)) + \frac{1}{3}(0 + V_1(5)) + \frac{1}{3}(0 + V_1(\text{Done})) \right) = \max(2, \frac{1}{3}(4 + 5 + 0)) = \max(2, 3) = 3$
  \]

- **State 3**:
  \[
  $V_2(3) = \max \left(3, \frac{1}{3}(0 + V_1(5)) + \frac{2}{3}(0 + V_1(\text{Done})) \right) = \max(3, \frac{1}{3} \cdot 5) = \max(3, \frac{5}{3}) = 3$
  \]

- **State 4**:
  \[
  $V_2(4) = 4 \quad (\text{same as before, always transitions to Done with Stop})$
  \]

- **State 5**:
  \[
  $V_2(5) = 5 \quad (\text{same as before, always transitions to Done with Stop})$
  \]

- **State Done**:
  \[
  $V_2(\text{Done}) = 0 \quad (\text{no further actions})$
  \]

#### Iteration 3 (\(k = 2\))

- **State 0**:
  \[
  $V_3(0) = \max \left(0, \frac{1}{3}(0 + V_2(2)) + \frac{1}{3}(0 + V_2(3)) + \frac{1}{3}(0 + V_2(4)) \right) = \max(0, \frac{1}{3}(3 + 3 + 4)) = \max(0, 10/3) \approx 3.33$
  \]

- **State 2**:
  \[
  $V_3(2) = \max \left(2, \frac{1}{3}(0 + V_2(4)) + \frac{1}{3}(0 + V_2(5)) + \frac{1}{3}(0 + V_2(\text{Done})) \right) = \max(2, \frac{1}{3}(4 + 5 + 0)) = \max(2, 3) = 3$
  \]

- **State 3**:
  \[
  $V_3(3) = \max \left(3, \frac{1}{3}(0 + V_2(5)) + \frac{2}{3}(0 + V_2(\text{Done})) \right) = \max(3, \frac{1}{3} \cdot 5) = \max(3, \frac{5}{3}) = 3$
  \]

- **State 4**:
  \[
 $ V_3(4) = 4 \quad (\text{same as before})$
  \]

- **State 5**:
  \[
  $V_3(5) = 5 \quad (\text{same as before})$
  \]

- **State Done**:
  \[
  $V_3(\text{Done}) = 0 \quad (\text{no further actions})$
  \]

#### Iteration 4 (\(k = 3\))

- **State 0**:
  \[
  $V_4(0) = \max \left(0, \frac{1}{3}(0 + V_3(2)) + \frac{1}{3}(0 + V_3(3)) + \frac{1}{3}(0 + V_3(4)) \right) = \max(0, \frac{1}{3}(3 + 3 + 4)) = \max(0, 10/3) \approx 3.33$
  \]

- **State 2**:
  \[
  $V_4(2) = \max \left(2, \frac{1}{3}(0 + V_3(4)) + \frac{1}{3}(0 + V_3(5)) + \frac{1}{3}(0 + V_3(\text{Done})) \right) = \max(2, \frac{1}{3}(4 + 5 + 0)) = \max(2, 3) = 3$
  \]

- **State 3**:
	$V_4(3) = \max \left(3, \frac{1}{3}(0 + V_3(5)) + \frac{2}{3}(0 + V_3(\text{Done})) \right) = \max(3, \frac{1}{3} \cdot 5) = \max(3, \frac{5}{3}) = 3$
  \]

- **State 4**: 
  $V_4(4) = 4 \quad (\text{same as before})$
  

- **State 5**:
  \[
  $V_4(5) = 5 \quad (\text{same as before})$
  \]

- **State Done**:
   $V_4(\text{Done}) = 0 \quad (\text{no further actions})$

### Summary of Value Iterations

| State | \(V_1(s)\) | \(V_2(s)\) | \(V_3(s)\) | \(V_4(s)\) |
|-------|------------|------------|------------|------------|
| 0     | 0          | 3          | 3.33       | 3.33       |
| 2     | 2          | 3          | 3          | 3          |
| 3     | 3          | 3          | 3          | 3          |
| 4     | 4          | 4          | 4          | 4          |
| 5     | 5          | 5          | 5          | 5          |
| Done  | 0          | 0          | 0          | 0          |


To find the optimal policy for the micro-blackjack problem, we need to use the optimal state values obtained from the value iteration process. The optimal policy will dictate the best action to take in each state to maximize the expected utility.

### Recap of Optimal Values from Value Iteration

Based on the value iteration process we completed, here are the state values after the fourth iteration:

| State | \(V^*(s)\) (Converged Value) |
|-------|-----------------------------|
| 0     | 3.33                        |
| 2     | 3                           |
| 3     | 3                           |
| 4     | 4                           |
| 5     | 5                           |
| Done  | 0                           |

### Steps to Find the Optimal Policy

The optimal policy \(\pi^*\) for each state \(s\) can be determined by choosing the action \(a\) that maximizes the expected utility based on the optimal state values \(V^*(s)\):

$\pi^*(s) = \arg\max_{a} \sum_{s'} T(s, a, s') \left[ R(s, a, s') + \gamma V^*(s') \right]$

We will evaluate this expression for each state to determine whether it is better to "Draw" or "Stop".

#### Calculating the Optimal Policy for Each State

**State 0:**

- **Action: Draw**
  - Possible next states: 2, 3, 4
  - Transition probabilities: \(T(0, \text{Draw}, s') = \frac{1}{3}\) for \(s' \in \{2, 3, 4\}\)
  - Expected utility:
  
  $Q(0, \text{Draw}) = \frac{1}{3} (0 + \gamma V^*(2)) + \frac{1}{3} (0 + \gamma V^*(3)) + \frac{1}{3} (0 + \gamma V^*(4))$
  $Q(0, \text{Draw}) = \frac{1}{3} (1 \times 3) + \frac{1}{3} (1 \times 3) + \frac{1}{3} (1 \times 4) = \frac{1}{3} (3 + 3 + 4) = \frac{10}{3} \approx 3.33$

- **Action: Stop**
  - The utility is the score of the state, \(R(0, \text{Stop}) = 0\)
  - Expected utility: 

  $Q(0, \text{Stop}) = R(0, \text{Stop}, \text{Done}) = 0$
  
- **Optimal action for state 0**: **Draw** (since 3.33 > 0)

**State 2:**

- **Action: Draw**
  - Possible next states: 4, 5, Done
  - Transition probabilities: \(T(2, \text{Draw}, s') = \frac{1}{3}\) for \(s' \in \{4, 5, \text{Done}\}\)
  - Expected utility:

  $Q(2, \text{Draw}) = \frac{1}{3} (1 \times 4) + \frac{1}{3} (1 \times 5) + \frac{1}{3} (1 \times 0) = \frac{1}{3} (4 + 5 + 0) = 3$

- **Action: Stop**
  - The utility is the score of the state, \(R(2, \text{Stop}, \text{Done}) = 2\)
  - Expected utility: 

  $Q(2, \text{Stop}) = 2$

- **Optimal action for state 2**: **Draw** (since 3 > 2)

**State 3:**

- **Action: Draw**
  - Possible next states: 5, Done
  - Transition probabilities: $(T(3, \text{Draw}, s') = \frac{1}{3}) for (s' = 5) and (\frac{2}{3}) for (s' = \text{Done})$
  - Expected utility:

  \[
  $Q(3, \text{Draw}) = \frac{1}{3} (1 \times 5) + \frac{2}{3} (1 \times 0) = \frac{5}{3} \approx 1.67$
  \]

- **Action: Stop**
  - The utility is the score of the state, \(R(3, \text{Stop}, \text{Done}) = 3\)
  - Expected utility: 

  \[
  $Q(3, \text{Stop}) = 3$
  \]

- **Optimal action for state 3**: **Stop** (since 3 > 1.67)

**State 4:**

- **Action: Draw**
  - The game ends if the player draws, resulting in a utility of 0.
  - Expected utility: 

  \[
  $Q(4, \text{Draw}) = 0$
  \]

- **Action: Stop**
  - The utility is the score of the state, \(R(4, \text{Stop}, \text{Done}) = 4\)
  - Expected utility: 

  \[
  $Q(4, \text{Stop}) = 4$
  \]

- **Optimal action for state 4**: **Stop** (since 4 > 0)

**State 5:**

- **Action: Draw**
  - The game ends if the player draws, resulting in a utility of 0.
  - Expected utility: 

  \[
  Q(5, \text{Draw}) = 0
  \]

- **Action: Stop**
  - The utility is the score of the state, \(R(5, \text{Stop}, \text{Done}) = 5\)
  - Expected utility: 

  \[
  Q(5, \text{Stop}) = 5
  \]

- **Optimal action for state 5**: **Stop** (since 5 > 0)

### Summary of the Optimal Policy

The optimal policy \(\pi^*\) for each state is as follows:

| State | Optimal Action (\(\pi^*(s)\)) |
|-------|-------------------------------|
| 0     | Draw                          |
| 2     | Draw                          |
| 3     | Stop                          |
| 4     | Stop                          |
| 5     | Stop                          |
| Done  | (No action, terminal state)   |

This policy dictates that you should continue drawing until the score reaches 3 or higher. At 3, 4, or 5, the optimal action is to stop to maximize the expected utility.


# Why Reinforcement Learning when we have MDP 
- **Role of RL:** Reinforcement learning utilizes MDPs to enable agents to learn from interactions with their environment to make optimal decisions. 
  
- **Challenge Addressed**: RL is particularly useful in situations where *the MDP is not fully known, requiring the agent to explore and learn from the environment*. 
  
- **Sparse Rewards:** A concept in RL where *rewards are not frequent* or informative, requiring careful design of the reward signal to enable learning. 
  
- **Key RL Terms:** 
	- **Model:** The mathematical representation of the environment's dynamics and rewards. 
	- **Policy:** A strategy that the agent employs, mapping states to actions. 
	- **Value Function:** A prediction of expected future rewards from a given state and action, under a specific policy. 

# Approaches to Reinforcement Learning 
Model-based and model-free learning are two approaches in the field of machine learning and reinforcement learning. They differ mainly in how they utilize information about the environment to learn and make decisions. 

1. **Model-Based Learning:** In model-based learning, the agent tries to learn a model of the environment and then uses this model to make decisions. 
   
	1. **Planning:** The agent can plan actions by considering future states and rewards, using its model of the environment. 
	   
	2. **Sample Efficiency:** Model-based methods can learn optimal policies with fewer interactions, as they can simulate and learn from different scenarios. Complexity: Learning a model can be complex, especially in environments with large state spaces or complex dynamics. 
	   
	3. **Utility Function:** Model-based RL systems often learn a utility function that assigns a value to each state in terms of the sum of future rewards. 
	   
	4. **Examples:** approx value iteration(unlike MDP, we dont know T and R), policy iteration 

2. **Model-Free Learning:** In model-free learning, the agent learns to make decisions without explicitly modeling the state-transition or reward functions. 
   
	1. **No Planning:** Actions are not planned based on a learned model; they are reactive based on the learned value function or policy. 
	   
	2. **Less Sample Efficiency:** Model-free methods are typically less sampleefficient as they do not simulate scenarios. 
	   
	3. **Simplicity:** These methods are simpler to implement and often used in environments with unknown or complex dynamics. 
	   
	4. **Examples:** TD Value Learning, Q-value Learning (improvement of TD learning)