
### 1. BERT (Bidirectional Encoder Representations from Transformers)**

- **Architecture**: BERT is an encoder-only transformer model.
- **Objective**: BERT's main goal is to generate deep bidirectional representations by conditioning on *both left and right contexts* in all layers. It achieves this through the tasks of Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).
- **Use Cases**: BERT is primarily used for tasks requiring a deep understanding of the context, like sentiment analysis, question answering, and named entity recognition.
---
### 2. GPT (Generative Pre-trained Transformer)

- **Architecture**: GPT is a decoder-only transformer model.
- **Objective**: GPT's main objective is to generate text by predicting the next word in a sequence, making it excellent for tasks involving text generation.
  - **Autoregressive Language Modeling**: GPT predicts the next word in a sequence given all previous words.
- **Use Cases**: GPT excels in tasks like text completion, dialogue generation, and any task that involves generating coherent text.

**Key Characteristics**:
- **Unidirectional**: GPT reads the text from left to right, meaning it conditions each word prediction on the *previous words only.*

---

### 3. T5 (Text-To-Text Transfer Transformer)

- **Architecture**: T5 uses both an encoder and a decoder, combining elements from both BERT and GPT.
- **Objective**: T5 reframes every NLP problem as a text-to-text problem, meaning both the input and output are text strings. For example, a translation task would take "Translate English to German: How are you?" and output "Wie geht es Ihnen?".
- **Unified Framework**: By casting all tasks as text-to-text, T5 simplifies the architecture and training process.
- **Use Cases**: T5 can be used for a wide range of tasks, including translation, summarization, and even question answering.

**Key Characteristics**:
- **Versatile**: Due to its text-to-text nature, T5 can be fine-tuned for virtually any NLP task.
- **Unified Approach**: By treating all tasks in the same way, T5 can leverage a single model architecture for diverse applications.

---
### How Encoder-Only and Decoder-Only Models Work Without the Other Component

#### Encoder-Only
- Encoder-only models don't need a decoder because their primary task is to understand and represent the input text. Since they don't generate new sequences but rather analyze the existing input, the decoder's generative capability is unnecessary.
	- **Inputs**:
	- **Text Sequence**: The primary input is a sequence of tokens (words or subwords) from the text. This sequence is typically tokenized and converted into numerical IDs.
	- **Token Type IDs** (Optional): In tasks involving sentence pairs (like Next Sentence Prediction in BERT), a token type ID may be used to distinguish between the tokens from different sentences.
	- **Position Embeddings**: Since transformers don’t inherently capture the order of tokens, position embeddings are added to indicate the position of each token in the sequence.
	- **Attention Mask**: This is a binary mask used to indicate which tokens should be attended to (1) and which should be ignored (0). It's especially useful when dealing with padding in batched inputs.
	
	**Outputs**:
	- **Contextualized Token Representations**: The output is typically a sequence of vectors, where each vector corresponds to a token in the input sequence. These vectors contain context-aware information about the tokens.
	  - **[CLS] Token Representation**: In BERT, the first token (often a special `[CLS]` token) can be used as a summary of the entire sequence for classification tasks.
	  - **Token-Level Outputs**: These are used for tasks like Named Entity Recognition (NER) or part-of-speech tagging.
	- **Class Labels** (after additional layers): For tasks like classification, the `[CLS]` token’s representation is usually fed into a linear layer followed by softmax to produce class probabilities.
	
	**Example**:
	- **Input**: `"The cat sat on the mat."`
	- **Output**: Contextualized vectors for each word in the sequence (e.g., `["The", "cat", "sat", "on", "the", "mat"]`), or a single vector representing the entire sentence.


#### Decoder-Only:
- Decoder-only models don't need an encoder because their primary task is sequence generation. They use the previously generated tokens as context for generating the next token. Since they generate text from scratch (or from a prompt), they don’t need the input encoding capability of the encoder.
	- - **Text Sequence** (or Prompt): The input is usually a sequence of tokens that the model uses to generate the next tokens. This sequence could be a few words, a sentence, or even a more extended prompt.
	- **Position Embeddings**: Similar to encoder-only models, position embeddings are added to capture the order of tokens in the sequence.
	- **Attention Mask**: In autoregressive models like GPT, a causal mask (often triangular) is applied to ensure that each token only attends to its preceding tokens (and not future tokens).
	
	**Outputs**:
	- **Next Token Prediction**: The output is typically the probability distribution over the vocabulary for the next token in the sequence. The model predicts the most likely next word given the input so far.
	- **Generated Text**: Through iterative predictions, the model can generate an entire sequence of text. Each new token prediction is added to the input, and the process repeats until a stopping criterion is met (e.g., reaching a special end-of-sequence token or a maximum length).
	
	**Example**:
	- **Input**: `"The cat"`
	- **Output**: `"sat"` (as the next token), and then with repeated predictions, it might generate the full sentence `"The cat sat on the mat."`