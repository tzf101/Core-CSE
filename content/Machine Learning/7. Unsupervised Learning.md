
# Mean, Median, Mode
### 1. **Mean**
- **Definition**: The mean, or arithmetic average, is the sum of all the data points divided by the number of data points.
- **Formula**: $\text{Mean} = \frac{\sum_{i=1}^{n} x_i}{n}$

### 2. **Median**
- **Definition**: The median is the middle value in a data set when the numbers are arranged in ascending or descending order. If the number of data points is odd, it's the middle number. If even, it's the average of the two middle numbers.
- **Steps to Find**:
	Arrange the data in ascending order.
	  1. If \( n \) (the number of data points) is odd, the median is the value at position \( $\frac{n+1}{2}$ \).
	  2. If \( n \) is even, the median is the average of the values at positions \( $\frac{n}{2}$ \) and \( $\frac{n}{2} + 1$ \).
- **Characteristics**: The median is less sensitive to outliers than the mean, making it a better measure of central tendency for skewed distributions.

### 3. **Mode**
- **Definition**: The mode is the value that appears most frequently in a data set. A data set can have more than one mode (bimodal or multimodal) or no mode at all if all values are unique.
- **Example**: For the data set \( [4, 1, 2, 2, 3, 4, 4, 5] \), the mode is 4, as it appears most frequently.
- **Characteristics**: The mode is the only measure of central tendency that can be used with nominal data (data that are categorized without a specific order).

### Summary:
- **Mean**: Best for symmetric distributions without outliers.
- **Median**: Best for skewed distributions or data with outliers.
- **Mode**: Useful for categorical data or when identifying the most common value.

# Partition Based Clustering
### K-Means
![[Pasted image 20240816224945.png]]


##### Features of K-means 
**Strength:** 
- Efficient: $O(tkn)$, where, n is # objects, k is # clusters, and t is # iterations. Normally, k, $t<n$
- Simplicity and ease of implementation
- Scalability 
**Weakness:** 
- Applicable only to objects in a continuous n-dimensional space 
- Initialization sensitivity 
- *Sensitive to noisy data and outliers*, an object with an extremely large value may substantially distort the distribution of the data 
- Not suitable to discover clusters with non-convex shapes. It Assumes spherical clusters with similar sizes: This can lead to poor results when the true clusters have different shapes, densities, or sizes. 
### K-medoids
- Select k points as the initial representative objects
- Repeat:
	- Assign each point to the closest medoid
	- Compute the total cost S of swapping the medoid m with a randomly selected object oi
	- If cost is less than 0, perform swap
![[Pasted image 20240816232135.png]]


**Weaknesses:**
- Less efficient than Kmeans for each iteration 
- Efficiency is $O(k(n-k)^2)$ where, n is # of data, k is # of clusters

### Choosing the number of clusters
![[Pasted image 20240816233945.png]]

#### Mean, Median, Mode
### 1. **Mean**
- **Definition**: The mean, or arithmetic average, is the sum of all the data points divided by the number of data points.
- **Formula**: $\text{Mean} = \frac{\sum_{i=1}^{n} x_i}{n}$

### 2. **Median**
- **Definition**: The median is the middle value in a data set when the numbers are arranged in ascending or descending order. If the number of data points is odd, it's the middle number. If even, it's the average of the two middle numbers.
- **Steps to Find**:
	Arrange the data in ascending order.
	  1. If \( n \) (the number of data points) is odd, the median is the value at position \( $\frac{n+1}{2}$ \).
	  2. If \( n \) is even, the median is the average of the values at positions \( $\frac{n}{2}$ \) and \( $\frac{n}{2} + 1$ \).
- **Characteristics**: The median is less sensitive to outliers than the mean, making it a better measure of central tendency for skewed distributions.

### 3. **Mode**
- **Definition**: The mode is the value that appears most frequently in a data set. A data set can have more than one mode (bimodal or multimodal) or no mode at all if all values are unique.
- **Example**: For the data set \( [4, 1, 2, 2, 3, 4, 4, 5] \), the mode is 4, as it appears most frequently.
- **Characteristics**: The mode is the only measure of central tendency that can be used with nominal data (data that are categorized without a specific order).

### Summary:
- **Mean**: Best for symmetric distributions without outliers.
- **Median**: Best for skewed distributions or data with outliers.
- **Mode**: Useful for categorical data or when identifying the most common value.

# Hierarchical Clustering
### Agglomerative Clustering(BIRCH)
This bottom-up approach starts with each object as a separate cluster and merges them into progressively larger clusters. The process typically follows these steps: 
- **Initialization:** Each data point is considered as an individual cluster. 
- **Merge Step:** The two closest clusters are merged to form a single cluster. This step is repeated until the desired number of clusters is reached or all data points are merged into a single cluster.
- **Termination:** The process ends either when all objects are in a single cluster or a predetermined number of clusters is reached. 
![[Pasted image 20240817011336.png]]
**Advantages:**
- Efficiency with Large Datasets: Ideal for scenarios with large data volumes. 
- Noise Handling: Capable of managing noisy dat
**Disadvantages:**
- Data Type Restriction: Best for numerical data; less suitable for categorical data. 
- Cluster Shape Limitation: Assumes roughly spherical clusters, which may not always be accurate. 
- Order Sensitivity: The insertion order impacts the CF tree and clusters.
### Divisive Clustering (Chameleon partially falls here)
This top-down approach starts with all data points in a single cluster and recursively splits the cluster into smaller ones. The steps are: 
- **Initialization:** All data points are considered as a single cluster. 
- **Split Step:** The cluster is split into smaller clusters based on a criterion like maximum distance between points. 
- **Termination:** The process ends when each data point becomes a separate cluster or when clusters meet certain criteria of similarity.
![[Pasted image 20240817011348.png]]
**Advantages:**
- **Adaptability:** Chameleon can adapt to different cluster shapes and densities, outperforming algorithms that assume clusters to be globular or of similar size. 
- **Noise and Outlier Handling:** Its design is inherently more robust against noise and outliers, owing to the local density-based approach used in the initial phase. Automatic Cluster
- **Detection:** Unlike many clustering algorithms, Chameleon automatically determines the number of clusters based on the dataset. 
**Limitations:**
- The iterative nature of Chameleon can lead to **high computational demand**, particularly for large or high-dimensional datasets during the initial graph construction. 
- **Parameter Sensitivity:** The performance of the algorithm can be heavily influenced by the choice of parameters, like the number of nearest neighbors used in constructing the K-NN graph

### Density Based Clustering (DBSCAN and OPTICS)
**Features:** Discovers clusters of arbitrary shape Handle noise 
1. **Eps (Îµ - Epsilon):** it sets the distance threshold within which points are considered to be in the same neighborhood. 
2. **MinPts (Minimum Points):** A point will only be considered a core point if there are at least MinPts points within its Eps-neighborhood, including the point itself. MinPts determines the minimum size of a cluster and helps to reduce the noise in the dataset. 
3. **NEps(p):** the set of points that lie within the Eps radius of point p. 
   ![[Pasted image 20240817020412.png]]

1. **Arbitrarily Select a Point from the Dataset**:  
   Start by selecting any point from the dataset.

2. **Retrieve All Points Density-Reachable from \( p \) with Respect to \( Eps \) and \( MinPts \)**:  
   This step involves finding all points that are within the \( Eps \)-neighborhood of \( p \). A point \( q \) is in the \( Eps \)-neighborhood of \( p \) if the distance between \( p \) and \( q \) is less than or equal to \( Eps \).

3. **Determine if \( p \) is a Core Point**:  
   If \( p \) is a core point (meaning that there are at least \( MinPts \) points within its \( Eps \)-neighborhood, including itself), then a cluster is formed. This cluster includes \( p \) and all points that are density-reachable from \( p \).  
   - If not, \( p \) could either be a border point or noise. If it's a border point, it might still belong to a cluster that a nearby core point forms. If it's noise, it won't be included in any cluster.

4. **Expand the Cluster**:  
   If \( p \) is a core point, the algorithm recursively retrieves all points density-reachable from each point in the cluster. This step involves checking the \( Eps \)-neighborhood of each new point added to the cluster and determining if it's also a core point. If so, the cluster expands to include all points density-reachable from this new core point.

5. **Processing Border Points**:  
   If the initially chosen point is a border point, no points are density-reachable from \( p \) under the DBSCAN criteria. In this case, \( p \) is either assigned to a cluster if it falls within the \( Eps \)-neighborhood of a core point of an already formed cluster or marked as noise.

6. **Continue Until All Points are Processed**:  
   The algorithm continues this process, selecting new points as starting points until every point in the dataset has been processed. Points that are not reachable from any core point are labeled as noise.

7. **Resulting Clusters**:  
   The output of DBSCAN is a set of clusters, each formed around a core point, and a collection of noise points that don't belong to any cluster.

# Support Vector Machine (SVM) 
Support Vector Machines (SVMs) are a type of supervised machine learning model used for classification and regression tasks. **SVMs are particularly well-suited for problems with high-dimensional data or complex decision boundaries**. 

The main idea behind SVMs is *to find the optimal hyperplane that separates data points of different classes with the maximum margin.* 

The margin is the distance between the hyperplane and the nearest data points from each class, called support vectors. The larger the margin, the better the generalization of the model, reducing the risk of overfitting.

![[Pasted image 20240816235753.png]]
![[Pasted image 20240816235841.png]]

This is a quadratic optimization problem where we are trying to find the weight vector \( $\mathbf{w}$ \) and bias \( $b$ \) that **minimize the norm of \( $\mathbf{w}$ \)**, while ensuring that all data points are correctly classified and lie on the correct side of the margin.

##### Lagrange Multipliers

To solve this constrained optimization problem, we introduce Lagrange multipliers \( $\alpha_i \geq 0$ \) for each of the constraints:

$L(\mathbf{w}, b, \alpha) = \frac{1}{2} \|\mathbf{w}\|^2 - \sum_{i=1}^N \alpha_i \left[ y_i (\mathbf{w} \cdot \mathbf{x}_i + b) - 1 \right]$

Here, \( $L(\mathbf{w}, b, \alpha)$ \) is the Lagrangian, and \( $\alpha_i$ \) are the Lagrange multipliers.

##### Deriving the Dual Problem

To derive the dual problem, we minimize the Lagrangian with respect to the primal variables \( $\mathbf{w}$ \) and \( b \). The dual problem involves maximizing the Lagrange function with respect to \( $\alpha$ \) after eliminating \( $\mathbf{w}$ \) and \( b \).

###### Step 1: Minimization with respect to \( $\mathbf{w}$ \) and \( b \)

To minimize \( $L(\mathbf{w}, b, \alpha)$ \) with respect to \( $\mathbf{w}$ \) and \( $b$ \), we take the partial derivatives and set them to zero:

1. **Partial derivative with respect to \( $\mathbf{w}$ \):**
   
   $\frac{\partial L}{\partial \mathbf{w}} = \mathbf{w} - \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i = 0$
   
   Solving for \( $\mathbf{w}$ \), we get:
   
   $\mathbf{w} = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i$
   

2. **Partial derivative with respect to \( b \):**
   
   $\frac{\partial L}{\partial b} = -\sum_{i=1}^N \alpha_i y_i = 0$
   
   Therefore:
   
   $\sum_{i=1}^N \alpha_i y_i = 0$
   

#### Step 2: Substitute \( $\mathbf{w}$ \) into the Lagrangian

Substitute \( $\mathbf{w} = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i$ \) back into the Lagrangian:

$L(\alpha) = \frac{1}{2} \left( \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i \right) \cdot \left( \sum_{j=1}^N \alpha_j y_j \mathbf{x}_j \right) - \sum_{i=1}^N \alpha_i \left[ y_i \left( \sum_{j=1}^N \alpha_j y_j \mathbf{x}_j \right) \cdot \mathbf{x}_i - 1 \right]$


Simplifying this expression gives:

$L(\alpha) = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (\mathbf{x}_i \cdot \mathbf{x}_j)$


#### Dual Problem Formulation

The dual problem is to maximize \( $L(\alpha)$ \) with respect to \( $\alpha$ \), subject to the constraints:

$\sum_{i=1}^N \alpha_i y_i = 0$

$\alpha_i \geq 0, \quad \forall i = 1, 2, \ldots, N$

So the dual problem is:

$\max_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (\mathbf{x}_i \cdot \mathbf{x}_j)$


#### Decision Function

Once the optimal \( $\alpha_i$ \) values are found by solving the dual problem, the weight vector \( $\mathbf{w}$ \) can be computed as:

$\mathbf{w} = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i$

The decision function for classifying a new data point \( $\mathbf{x}$ \) is:

$f(\mathbf{x}) = \text{sign}\left( \mathbf{w} \cdot \mathbf{x} + b \right) = \text{sign}\left( \sum_{i=1}^N \alpha_i y_i (\mathbf{x}_i \cdot \mathbf{x}) + b \right)$

### Why Use the Dual Form?

1. **Kernels**: The dual form is particularly useful when applying kernel functions. By replacing the dot product \( $\mathbf{x}_i \cdot \mathbf{x}_j$ \) with a kernel function \( $K(\mathbf{x}_i, \mathbf{x}_j)$ \), SVM can classify non-linear data without explicitly mapping it into a high-dimensional space.

2. **Computational Efficiency**: When the number of features \( d \) is very large, solving the primal problem can be computationally expensive. The dual form operates in terms of the number of training examples \( N \), which can be smaller than \( d \) in some cases.

In summary, the dual form reformulates the original problem by focusing on the Lagrange multipliers associated with the constraints. This allows for the use of kernel functions and provides an alternative approach that can be computationally advantageous in certain scenarios.


## Non-Linear SVM

When the data is not linearly separable, SVM can still be applied by mapping the input data into a higher-dimensional space where a linear separator might exist. This is done using kernel functions.

#### Kernel Trick
A kernel function \( $K(\mathbf{x}_i, \mathbf{x}_j$) \) is used to compute the dot product of two vectors in the high-dimensional feature space without explicitly computing the mapping. The kernel function allows SVM to operate in the original input space while implicitly considering the higher-dimensional feature space.

Common kernel functions include:

- **Polynomial Kernel**: \($K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i \cdot \mathbf{x}_j + 1)^p$ \)
- **Gaussian (RBF) Kernel**: \( $K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2\sigma^2}\right)$ \)
- **Sigmoid Kernel**: \( $K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\kappa \mathbf{x}_i \cdot \mathbf{x}_j + \theta)$ \)

### Dual Form with Kernel
The dual form of the optimization problem for non-linear SVM is:

$\max_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j K(\mathbf{x}_i, \mathbf{x}_j)$

subject to:
\
$\sum_{i=1}^N \alpha_i y_i = 0$

$\alpha_i \geq 0, \quad \forall i = 1, 2, \ldots, N$
\

### Decision Function with Kernel
The decision function in the non-linear case is:

$f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^N \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b\right)$

In summary, SVM constructs a hyperplane (or a set of hyperplanes in the case of non-linear data) in a high-dimensional space to separate different classes with the maximum margin. The kernel trick enables SVM to handle non-linear data by implicitly mapping it to a higher-dimensional space where linear separation is feasible.