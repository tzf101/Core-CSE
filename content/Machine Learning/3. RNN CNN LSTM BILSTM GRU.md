### 1. Recurrent Neural Networks (RNNs)

**Overview:**
Recurrent Neural Networks (RNNs) are a class of neural networks designed to recognize patterns in sequences of data, such as time series, speech, or text. They are particularly well-suited for tasks where the order of inputs is significant, as they can maintain a memory of previous inputs. [RNN GFG](https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/)
![[Pasted image 20240813122109.png]]

- **Sequential Processing:** At each time step, an RNN takes an input (e.g., a word or a data point) and combines it with the hidden state from the previous time step. The new hidden state is then passed on to the next time step.
- **Recurrence:** The hidden state is updated recurrently, allowing the network to maintain a memory of the sequence.

#### Challenges:
- **Vanishing and Exploding Gradients:** When training RNNs with backpropagation through time (BPTT), gradients can either vanish (become too small) or explode (become too large), making training difficult.
- **Short-Term Memory:** Standard RNNs *struggle with long-term dependencies*, where important information from earlier in the sequence is needed to make decisions later on.

### 2. Long Short-Term Memory Networks (LSTMs)

**Overview:**
Long Short-Term Memory (LSTM) networks are a type of RNN designed to address the shortcomings of traditional RNNs, particularly in dealing with long-term dependencies. LSTMs introduce a more complex structure that enables them to remember information over longer periods.
![[Pasted image 20240813152653.png|A single LSTM cell]]
#### Structure:
- **Cell State:** The LSTM's memory is managed by a cell state, which runs through the entire sequence, allowing information to be retained over long time steps.
- **Gates:** LSTMs have three gates (input, forget, and output) that regulate the flow of information into and out of the cell state.
  - **Forget Gate:** Decides what information to discard from the cell state.
  - **Input Gate:** Controls what new information is added to the cell state.
  - **Output Gate:** Determines what information is passed to the next hidden state.

#### How LSTMs Work:
- **Memory Management:** The cell state is modified at each time step based on the information passing through the gates, allowing the network to keep or forget information as needed.
- **Handling Long-Term Dependencies:** The careful management of memory enables LSTMs to maintain relevant information over long sequences, overcoming the vanishing gradient problem.

#### Applications:
- **Text Generation:** Generating text sequences that follow the style of the input text.
- **Speech Recognition:** Converting spoken words into text while retaining context over time.
- **Time Series Forecasting:** Predicting future values by capturing long-term dependencies in the data.
### 3. BiLSTM
BiLSTM adds one more LSTM layer, which reverses the direction of information flow. Briefly, it means that the input sequence flows backward in the additional LSTM layer. Then we combine the outputs from both LSTM layers in several ways, such as average, sum, multiplication, or concatenation.
![[Pasted image 20240815135344.png]]
**Why is BiLSTM better than LSTM ?**
The main reason is that every component of an input sequence has information from both the past and present. For this reason, BiLSTM can produce a more meaningful output, combining LSTM layers from both directions.

For example, the sentence:

> Apple is something that…

might be about the apple as fruit or about the company Apple. Thus, LSTM doesn’t know what “Apple” means, since it doesn’t know the context from the future.

In contrast, most likely in both of these sentences:

> Apple is something that competitors simply cannot reproduce.

and

> Apple is something that I like to eat.

BiLSTM will have a different output for every component (word) of the sequence (sentence). As a result**, the BiLSTM model is beneficial in some NLP tasks, such as sentence classification, translation, and entity recognition. In addition, it finds its applications in speech recognition, protein structure prediction, handwritten recognition, and similar fields.**

Finally, regarding the disadvantages of BiLSTM compared to LSTM, it’s worth mentioning that BiLSTM is a much slower model and requires more time for training. Thus, we recommend using it only if there’s a real necessity.

### 4. GRU
![[Pasted image 20240815140910.png]]
- GRU has a *simpler architecture than LSTM*, with fewer parameters, which can make it easier to train and more computationally efficient.
- In LSTM, the memory cell state is maintained separately from the hidden state and is updated using three gates: the input gate, output gate, and forget gate. *In GRU, the memory cell state is replaced with a “candidate activation vector,”* which is updated using two gates: the reset gate and update gate.
	![[Pasted image 20240815140910.png]]
- **Reset gate:** The reset gate determines how much of the previous hidden state to forget. It takes as input the previous hidden state and the current input, and produces a vector of numbers between 0 and 1 that controls the degree to which the previous hidden state is “reset” at the current time step.
- **Update gate:** The update gate determines how much of the candidate activation vector to incorporate into the new hidden state. It takes as input the previous hidden state and the current input, and produces a vector of numbers between 0 and 1 that controls the degree to which the candidate activation vector is incorporated into the new hidden state.
- **Candidate activation vector:** The candidate activation vector is a modified version of the previous hidden state that is “reset” by the reset gate and combined with the current input. It is computed using a tanh activation function that squashes its output between -1 and 1.
- **Output layer:** The output layer takes the final hidden state as input and produces the network’s output. This could be a single number, a sequence of numbers, or a probability distribution over classes, depending on the task at hand.

### 5. Convolutional Neural Networks (CNNs)

Convolutional Neural Networks (CNNs) are a class of neural networks primarily used for processing grid-like data, such as images. They are designed to automatically and adaptively learn spatial hierarchies of features from input data.

#### Simple CNN Architecture
1. **Input layer:** Takes the input image of a fixed size. 
2. **Convolutional layer:** Applies a set of filters to extract features. 
3. **Activation function:** Applies ReLU to introduce non-linearity.
4. **Pooling layer:** Reduces the spatial dimensions of the feature maps. 
5. **Convolutional layer:** Applies another set of filters to extract more complex features. 
6. **Activation function:** Applies ReLU again. 
7. **Pooling layer:** Reduces the spatial dimensions of the feature maps. 
8. **Flatten:** Unfolds the feature maps into a single vector. 
9. **Fully connected layer:** Combines the features and makes the final decision for the classification task.
10. **Softmax layer:** Assigns a probability to each class, and the class with the highest probability is chosen as the final prediction.
#### How CNNs Work:
- **Feature Detection:** The convolutional layers automatically detect various features in the input (like edges, textures, and shapes in images) as the network learns.
- **Hierarchical Learning:** Lower layers detect simple patterns, while higher layers capture more complex patterns by combining simpler ones.
- **Translation Invariance:** CNNs are effective in recognizing patterns regardless of their position in the input due to the convolutional process.

#### Applications:
- **Image Classification:** Categorizing images into predefined classes (e.g., identifying objects in images).
- **Object Detection:** Locating and identifying objects within an image.
- **Video Analysis:** Analyzing video frames to recognize actions or events.

### Filters in CNN

In Convolutional Neural Networks (CNNs), **filters** (also known as **kernels**) are small matrices that slide over the input data (such as an image) to detect specific patterns or features. Here are some examples of common filters used in CNNs:

1. **Edge Detection Filters**
   - **Sobel Filter:**
     - **Purpose:** Detects edges in an image by calculating the gradient of the pixel intensity in a particular direction (usually horizontal or vertical).
     - **Example Kernels:**
       - Horizontal Sobel Filter: $$\begin{bmatrix}
         -1 & 0 & 1 \\
         -2 & 0 & 2 \\
         -1 & 0 & 1
         \end{bmatrix}$$
       - Vertical Sobel Filter: $$\begin{bmatrix}
         -1 & -2 & -1 \\
          0 &  0 &  0 \\
          1 &  2 &  1
         \end{bmatrix}$$
     - **Functionality:** The Sobel filter emphasizes the horizontal or vertical edges, helping to detect where there is a sharp change in intensity.

   - **Prewitt Filter:**
     - **Purpose:** Similar to the Sobel filter, it is used for edge detection by computing the gradient of the image intensity.
     - **Example Kernels:**
       - Horizontal Prewitt Filter: $$\begin{bmatrix}
         -1 & 0 & 1 \\
         -1 & 0 & 1 \\
         -1 & 0 & 1
         \end{bmatrix}$$
       - Vertical Prewitt Filter: $$\begin{bmatrix}
         -1 & -1 & -1 \\
          0 &  0 &  0 \\
          1 &  1 &  1
         \end{bmatrix}$$
     - **Functionality:** Like the Sobel filter, it detects edges by emphasizing changes in intensity, though it is less sensitive to noise.

2. **Sharpening Filters**
   - **Purpose:** Enhances the contrast of edges in an image, making it appear sharper.
   - **Example Kernel: $$\begin{bmatrix}
      0 & -1 &  0 \\
     -1 &  5 & -1 \\
      0 & -1 &  0
     \end{bmatrix}$$
   - **Functionality:** The center of the filter has a higher positive value, which increases the intensity of the pixel, while the surrounding negative values subtract from the neighboring pixels, enhancing edges.

3. **Blur Filters**
   - **Gaussian Blur Filter:**
     - **Purpose:** Reduces noise and detail in an image by averaging the pixels in a neighborhood.
     - **Example Kernel:** $$\frac{1}{16}
       \begin{bmatrix}
       1 & 2 & 1 \\
       2 & 4 & 2 \\
       1 & 2 & 1
       \end{bmatrix}$$
     - **Functionality:** The Gaussian filter smooths the image by averaging the pixel values within the kernel, giving more weight to the central pixels.

   - **Box Blur Filter:**
     - **Purpose:** Another type of averaging filter that smooths the image by averaging the pixels equally.
     - **Example Kernel:**  $$\frac{1}{9}
       \begin{bmatrix}
       1 & 1 & 1 \\
       1 & 1 & 1 \\
       1 & 1 & 1
       \end{bmatrix}$$
     - **Functionality:** The Box Blur filter averages the pixel values within the kernel equally, leading to a uniform blur effect.

4. **Embossing Filters**
   - **Purpose:** Creates a 3D effect by highlighting edges with a shadow.
   - **Example Kernel: $$\begin{bmatrix}
     -2 & -1 & 0 \\
     -1 &  1 & 1 \\
      0 &  1 & 2
     \end{bmatrix}$$
   - **Functionality:** The Emboss filter highlights the transitions between light and dark, creating a shadowed effect that gives the image a raised or embossed look.

5. **Identity Filter**
   - **Purpose:** Leaves the image unchanged, often used as a baseline or for debugging purposes.
   - **Example Kernel:** $$\begin{bmatrix}
     0 & 0 & 0 \\
     0 & 1 & 0 \\
     0 & 0 & 0
     \end{bmatrix}$$
   - **Functionality:** The identity filter passes the input through unchanged, effectively producing the same image as the input.
### ResNet 
ResNet, or Residual Network, is a type of deep neural network architecture to address the challenges faced when *training deeper neural networks*. There are two main problems with deeper networks: 

1. **They are harder to train:** As the number of layers increases, the network becomes more complex, and the training error may increase again after initially decreasing. *This is known as the degradation problem.* 
2. **Exploding and vanishing gradients:** When training deep networks using back-propagation, the gradients can become very small (vanish) or very large (explode) as they are propagated through the layers. 

The benefit of training a residual network is that even if we train deeper networks, the training error does not increase.

![[Pasted image 20240815222937.png]]

This makes it difficult for the network to learn and update its weights effectively. ResNet addresses these problems by introducing skip connections (also known as residual connections or shortcut connections) that allow the output of one layer to be added to the output of another layer deeper in the network (Typically, the output of a layer is added to the output of a layer two layers away (i.e., skipping one layer) or more, depending on the architecture). This creates a more direct path for the gradients to flow through during back-propagation, which helps mitigate the issues of vanishing and exploding gradients.

### Inception Network 

The motivation of the inception network is, rather than requiring us to pick the filter size manually, let the network decide what is best to put in a layer. We give it choices and hopefully it will pick up what is best to use in that layer.

![[Pasted image 20240816004633.png]]

- The Inception network is designed to address the problem of computational efficiency and overfitting in deep neural networks. 
- It achieves this by introducing the concept of "inception modules" that perform multiple convolutions and pooling operations in parallel, allowing the network to capture features at different scales and resolutions. 
- The key innovation of Inception Networks is the "Inception module", a building block that enables the network to learn from multiple scales and complexities of features simultaneously.
- Inception modules consist of parallel convolutional layers with different filter sizes (e.g., 1x1, 3x3, and 5x5), as well as pooling layers, which are then concatenated. 
- This approach allows the model to learn more complex and spatially hierarchical features, resulting in better performance.