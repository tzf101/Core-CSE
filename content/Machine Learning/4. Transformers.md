# What are Transformers
- Transformers are meant to solve any task that transforms an input sequence to an output sequence. This is why they are called “Transformers”.
- Transformers were *inspired by the encoder-decoder* architecture found in RNNs. However, Instead of using recurrence, the Transformer model is completely *based on the Attention mechanism.*
- So, what are RNN/LSTMs main problems? They are quite ineffective for NLP tasks for two main reasons:
	- They process the input data sequentially, one after the other. Such a recurrent process does not make use of modern graphics processing units (GPUs), which were designed for parallel computation and, thus, makes the training of such models quite slow.
	- *They become quite ineffective when elements are distant from one another*. This is due to the fact that information is passed at each step and the longer the chain is, the more probable the information is lost along the chain.
	- Transformer's Attention mechanism improves these by:
		- Pay attention to specific words, no matter how distant they are.
		- Boost the performance speed.

### The Encoder WorkFlow
 The primary function of the encoder is to transform the input tokens into contextualized representations. Unlike earlier models that processed tokens independently, the Transformer encoder captures the context of each token with respect to the entire sequence.
![[Pasted image 20240813214936.png]]
#### STEP 1 - Input Embeddings

The embedding only happens in the bottom-most encoder. The encoder begins by converting input tokens - words or subwords - into vectors using embedding layers. These embeddings capture the semantic meaning of the tokens and convert them into numerical vectors.

All the encoders receive a list of vectors, each of size 512 (fixed-sized). In the bottom encoder, that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below them.
#### STEP 2 - Positional Encoding

Since Transformers do not have a recurrence mechanism like RNNs, they use positional encodings added to the input embeddings to provide information about the position of each token in the sequence. 
This is done using a combination of various sine and cosine functions to create positional vectors, enabling the use of this positional encoder for sentences of any length.
#### STEP 3 - Stack of Encoder Layers

This layer comprises two sub-modules:
- A multi-headed attention mechanism.
- A fully connected network.

##### STEP 3.1 Multi-Headed Self-Attention Mechanism

In the encoder, the multi-headed attention utilizes a specialized attention mechanism known as self-attention. This approach enables the models to relate each word in the input with other words. For instance, in a given example, the model might learn to connect the word “are” with “you”.

This mechanism allows the encoder to focus on different parts of the input sequence as it processes each token. It computes attention scores based on:

- A *query* is a vector that represents a specific word or token from the input sequence in the attention mechanism.

- A *key* is also a vector in the attention mechanism, corresponding to each word or token in the input sequence.

- Each value is associated with a key and is used to construct the output of the attention layer. *When a query and a key match well, which basically means that they have a high attention score*, the corresponding value is emphasized in the output.

	**Why it’s called Multi-Head Attention?**
	Remember that before all the process starts, we break our queries, keys and values h times. This process, known as self-attention, happens separately in each of these smaller stages or 'heads'. Each head works its magic independently, conjuring up an output vector.his first Self-Attention module enables the model to capture contextual information from the entire sequence. Instead of performing a single attention function, queries, keys and values are linearly projected h times. On each of these projected versions of queries, keys and values the attention mechanism is performed in parallel, yielding h-dimensional output values. The score matrix establishes the degree of emphasis each word should place on other words. Therefore, each word is assigned a score in relation to other words within the same time step. A higher score indicates greater focus.

- Once the query, key, and value vectors are passed through a linear layer, a dot product matrix multiplication is performed between the queries and keys, resulting in the creation of a score matrix.
- The scores are then scaled down by dividing them by the square root of the dimension of the query and key vectors. This step is implemented to ensure more stable gradients, as the multiplication of values can lead to excessively large effects.
- The following step of the attention mechanism is that weights derived from the softmax function are multiplied by the value vector, resulting in an output vector.

#### STEP 4 - Output of the Encoder

The output of the final encoder layer is a set of vectors, each representing the input sequence with a rich contextual understanding.

### The Decoder WorkFlow

The decoder's role centers on crafting text sequences.
![[Pasted image 20240814011122.png]]
Softmax takes a vector of real numbers (typically the output scores from a neural network) and transforms it into a probability distribution. The key properties of this probability distribution are:
1. **Non-Negativity**: All the output probabilities are non-negative.
2. **Sum to One**: The probabilities sum to 1, making it a valid probability distribution.
	$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}$
The final of the decoder's process involves a linear layer, serving as a classifier, topped off with a softmax function to calculate the probabilities of different words. 

The decoder operates in an autoregressive manner, kickstarting its process with a start token. It cleverly uses a list of previously generated outputs as its inputs, in tandem with the outputs from the encoder that are rich with attention information from the initial input.

This sequential dance of decoding continues until the decoder reaches a pivotal moment: the generation of a token that signals the end of its output creation.

#### STEP 1 - Output Embeddings

At the decoder's starting line, the process mirrors that of the encoder. Here, the input first passes through an embedding layer. 
	- **Encoder Input:** The encoder typically processes the input sequence, such as a sentence in one language if you're doing machine translation. The words or tokens in this input sequence are converted into embeddings by the encoder's embedding layer.
	- **Decoder Input:** The decoder generates the output sequence, which could be a sentence in another language. The decoder needs to process the output sequence generated so far (during training, this includes the target sequence, shifted by one position). The tokens in this sequence are also converted into embeddings by the decoder's embedding layer.
#### STEP 2 - Positional Encoding

Following the embedding, again just like the decoder, the input passes by the positional encoding layer.
#### STEP 3 - Stack of Decoder Layers
##### STEP 3.1 Masked Self-Attention Mechanism
Similar to self-attention of encoder but this masking ensures that the predictions for a particular position can only depend on known outputs at positions before it.
##### STEP 3.2 - Encoder-Decoder Multi-Head Attention or Cross Attention
Here, the outputs from the encoder take on the roles of both queries and keys, while the outputs from the first multi-headed attention layer of the decoder serve as values.
##### STEP 3.3 Feed-Forward Neural Network
Similar to the encoder, each decoder layer includes a fully connected feed-forward network, applied to each position separately and identically.
#### STEP 4 Linear Classifier and Softmax for Generating Output Probabilities

The journey of data through the transformer model culminates in its passage through a final linear layer, which functions as a classifier.
![[Pasted image 20240814121027.png]]
##### Normalization and Residual Connections

Each sub-layer (masked self-attention, encoder-decoder attention, feed-forward network) is followed by a normalization step, and each also includes a residual connection around it.

##### Output of the Decoder

The final layer's output is transformed into a predicted sequence, typically through a linear layer followed by a softmax to generate probabilities over the vocabulary.
![[Pasted image 20240814124526.png]]