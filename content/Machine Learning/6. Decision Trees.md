- Commonly used in machine learning, statistics, and data mining for classification and regression tasks.

- The entropy in decision trees is a measure of the impurity or uncertainty in a dataset and is used to determine the best attribute to split the data at each step. 

- The formula for entropy H(S) for a dataset S with n classes is: $H(S) = -\sum_{i=1}^{n} p_i \log_2(p_i)$
  Where:
	- $p_i$ is the proportion of elements in class $i$ in the dataset $S$.
	- $n$ is the number of classes.

- This formula calculates the weighted sum of the negative log probabilities of each class, reflecting the average level of uncertainty in the dataset. If the dataset is perfectly homogeneous (i.e., all data points belong to the same class), the entropy is 0. Conversely, the entropy is highest when the data points are evenly distributed across all classes.

- The information gain is calculated using the formula: 
	
	$Gain(S,T) = Entropy(S) – Entropy(S,T)$
	
	Perform the split, for which we get the highest information gain 

### Classification Tree
![[Pasted image 20240816162145.png]]
### Regression Tree
It is almost the same as classification tree but the impurity measure is replaced by MSE to use it in regression.
![[Pasted image 20240816162203.png]]

# Evaluating a System

- Accuracy → How accurate my predictions are 
- Recall → How much of the actual positive values can we detect 
- Precision → How accurate my positively detected decisions are 
**Formulas:**
- $\text{Accuracy} = \frac{P +N} {TP +TN}$ 
- $\text{Recall} = \frac{P} {TP}$
- $\text{Precision} = \frac{TP} {TP + FP}$

### Principal Component Analysis (PCA) 
PCA transforms a high-dimensional dataset into a lower-dimensional one while retaining as much variance as possible.

### 1. **Standardize the Data**
   - **Why?** PCA is sensitive to the variances of the original variables, so the data needs to be standardized to ensure that each variable contributes equally.
   - **How?** Subtract the mean of each variable from the dataset and divide by the standard deviation, resulting in a dataset with a mean of 0 and a standard deviation of 1.
	   $z = \frac{x - \mu}{\sigma}$
	   where \( $x$\) is the original data, $\mu$ is the mean, and \( $\sigma$\) is the standard deviation.

### 2. **Compute the Covariance Matrix**
   - **Why?** The covariance matrix captures the relationships between different variables, showing how they vary together.
   - **How?** Calculate the covariance matrix of the standardized data. If the dataset has \( p \) variables, the covariance matrix will be \( $p \times p$ \) in size.
	   $\text{Cov}(X) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(X_i - \bar{X})^T$
	   where \( X \) is the standardized data matrix, \( $\bar{X}$ \) is the mean, and \( $n$ \) is the number of observations.

### 3. **Compute the Eigenvalues and Eigenvectors**
   - **Why?** Eigenvalues and eigenvectors of the covariance matrix identify the directions (principal components) in which the data varies the most.
   - **How?** Solve the eigenvalue equation:
	   $Cov(X) v = \lambda v$
	   where \( $\lambda$ \) are the eigenvalues and \( $v$ \) are the eigenvectors. Each eigenvector corresponds to a principal component, and the associated eigenvalue indicates the variance explained by that component.

### 4. **Sort Eigenvalues and Eigenvectors**
   - **Why?** The principal components are ordered by the amount of variance they explain, which is given by the eigenvalues.
   - **How?** Sort the eigenvalues in descending order and rearrange the eigenvectors accordingly. The eigenvector with the highest eigenvalue is the first principal component, the second highest is the second principal component, and so on.

### 5. **Choose the Number of Principal Components**
   - **Why?** The goal is to reduce dimensionality while retaining as much variance as possible. The number of components is chosen based on the cumulative explained variance.
   - **How?** Sum the sorted eigenvalues and calculate the cumulative explained variance ratio:
	   $\text{Cumulative variance} = \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{p} \lambda_i}$
	   Choose the smallest number \( $k$ \) such that the cumulative variance is above a certain threshold (e.g., 90-95%).

### 6. **Project the Data onto the Principal Components**
   - **Why?** To reduce the dimensionality of the dataset by expressing it in terms of the principal components.
   - **How?** Multiply the original standardized data by the selected eigenvectors (principal components):
	   $Y = X \cdot V$
	   where \( Y \) is the transformed data, \( X \) is the standardized original data, and \( V \) is the matrix of selected eigenvectors.

### 7. **Interpret the Results**
   - **Why?** Understanding what each principal component represents can provide insights into the structure of the data.
   - **How?** Analyze the principal components by looking at the loadings (coefficients) of the original variables in each component. This can help in identifying the most influential variables.

### 8. **Reconstruction (Optional)**
   - **Why?** To assess the quality of the dimensionality reduction.
   - **How?** Reconstruct the original data using the selected principal components and compare it with the original data to evaluate the information loss.
	   $X_{\text{reconstructed}} = Y \cdot V^T$
