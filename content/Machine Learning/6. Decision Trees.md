- Commonly used in machine learning, statistics, and data mining for classification and regression tasks.

- The entropy in decision trees is a measure of the impurity or uncertainty in a dataset and is used to determine the best attribute to split the data at each step. 

- The formula for entropy H(S) for a dataset S with n classes is: $H(S) = -\sum_{i=1}^{n} p_i \log_2(p_i)$
  Where:
	- $p_i$ is the proportion of elements in class $i$ in the dataset $S$.
	- $n$ is the number of classes.

- This formula calculates the weighted sum of the negative log probabilities of each class, reflecting the average level of uncertainty in the dataset. If the dataset is perfectly homogeneous (i.e., all data points belong to the same class), the entropy is 0. Conversely, the entropy is highest when the data points are evenly distributed across all classes.

- The information gain is calculated using the formula: 
	
	$Gain(S,T) = Entropy(S) – Entropy(S,T)$
	
	Perform the split, for which we get the highest information gain 

### Classification Tree
![[Pasted image 20240816162145.png]]
### Regression Tree
It is almost the same as classification tree but the impurity measure is replaced by MSE to use it in regression.
![[Pasted image 20240816162203.png]]

# Evaluating a System

- Accuracy → How accurate my predictions are 
- Recall → How much of the actual positive values can we detect 
- Precision → How accurate my positively detected decisions are 
**Formulas:**
- $\text{Accuracy} = \frac{P +N} {TP +TN}$ 
- $\text{Recall} = \frac{P} {TP}$
- $\text{Precision} = \frac{TP} {TP + FP}$
  