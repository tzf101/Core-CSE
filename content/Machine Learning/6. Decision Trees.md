- Commonly used in machine learning, statistics, and data mining for classification and regression tasks.
- The entropy in decision trees is a measure of the impurity or uncertainty in a dataset and is used to determine the best attribute to split the data at each step. 
- The formula for entropy H(S) for a dataset S with n classes is: $H(S) = -\sum_{i=1}^{n} p_i \log_2(p_i)$
  Where:
	- $p_i$ is the proportion of elements in class $i$ in the dataset $S$.
	- $n$ is the number of classes.

- This formula calculates the weighted sum of the negative log probabilities of each class, reflecting the average level of uncertainty in the dataset. If the dataset is perfectly homogeneous (i.e., all data points belong to the same class), the entropy is 0. Conversely, the entropy is highest when the data points are evenly distributed across all classes.
- The information gain is calculated using the formula: Gain(S,T) = Entropy(S) â€“ Entropy(S,T)
  Perform the split, for which we get the highest information gain 