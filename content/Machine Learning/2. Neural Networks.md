# Neural Networks
Inspired by the human brain, neural networks are composed of interconnected nodes called neurons, which are organized into layers. 

1. **Structure of Neural Networks**:
	   - **Neurons (Nodes)**: Each neuron receives input, processes it, and passes the result to the next layer. In a biological context, this is analogous to a nerve cell in the brain.
	   - **Layers**:
	     - **Input Layer**: The first layer in a neural network that receives the initial data.
	     - **Hidden Layers**: These layers sit between the input and output layers. They perform computations and learn features from the data. The term "hidden" simply means that these layers are not directly visible in the input or output.
	     - **Output Layer**: The final layer, which produces the network's output based on the learned patterns. Fully connected layers are used to interpret these features and make a decision. For example, in an image classification task, after the convolutional and pooling layers have detected various patterns and features, the fully connected layer will combine this information to decide which class the image belongs to.

2. **Functioning of Neural Networks**:
	   - **Weights and Biases**: Each connection between neurons has a weight, which determines the importance of the input value. Neurons also have a bias that allows the model to be shifted in various directions.
	   - **Activation Functions**: After computing a weighted sum of the inputs, the neuron applies an activation function. Common activation functions include the sigmoid, tanh, and ReLU (Rectified Linear Unit). These functions introduce non-linearity, allowing the network to learn complex patterns.
	   - **Forward Propagation**: In this process, data is passed through the network from the input layer to the output layer. The output is compared to the expected result (in supervised learning) to calculate the error.
	   - **Backpropagation**: This is the process of adjusting the weights and biases based on the error calculated during forward propagation. By minimizing the error across many iterations, the network "learns" the optimal weights for making predictions. [Explanation](https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3)

3. **Types of Neural Networks**:
	   - **Feedforward Neural Networks (FNNs)**: The simplest type, where connections between the nodes do not form cycles. Information moves in one direction, from input to output.
	   - **Convolutional Neural Networks (CNNs)**: Primarily used in image processing, CNNs use convolutional layers to automatically detect and learn spatial hierarchies of features from input images.
	   - **Recurrent Neural Networks (RNNs)**: Designed for sequential data, RNNs have connections that form loops, allowing information to persist, making them ideal for tasks like language modeling and time-series prediction.
	   - **Generative Adversarial Networks (GANs)**: A type of neural network where two networks, a generator and a discriminator, are trained simultaneously, often used for generating realistic data like images or music.

4. **Training a Neural Network**:
	   - **Data**: Neural networks require large amounts of labeled data for supervised learning tasks.
	   - **Learning Rate**: A crucial hyper-parameter that controls how much to change the model in response to the estimated error each time the model weights are updated.
	   - **Epochs**: One complete pass of the training data through the neural network. Multiple epochs are often required to train a model effectively.
	   - **Loss Function**: A function that measures the difference between the network’s output and the actual target value. Common loss functions include mean squared error (MSE) for regression tasks and cross-entropy for classification tasks.
	   - **Optimization Algorithm**: Algorithms like Stochastic Gradient Descent (SGD), Adam, and RMSprop are used to minimize the loss function by updating the weights and biases.
# Debugging ML Algorithms
![[Pasted image 20240815192105.png]]

#### Bias
- **Definition**: Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a much simpler model. It is the error due to overly simplistic assumptions in the learning algorithm.
- **High Bias**: Models with high bias are often too simple, which leads to underfitting. This means that the model performs poorly on both the training data and unseen data because it cannot capture the underlying patterns in the data.
- **Example:** A linear model trying to fit a highly non-linear relationship will have high bias.

#### Variance
- **Definition**: Variance refers to the model's sensitivity to small fluctuations in the training data. A model with high variance pays too much attention to the training data and captures noise along with the underlying data pattern.
- **High Variance**: Models with high variance are often too complex, which leads to overfitting. This means that while the model performs well on training data, it performs poorly on unseen data because it has learned to *fit the noise* in the training data rather than the actual pattern.
- **Example:** A high-degree polynomial model might fit the training data perfectly but will likely fail on new data due to overfitting.

#### Bias-Variance Trade-off
- **Trade-off**: There is a trade-off between bias and variance. If you make the model more complex (e.g., by adding more features or using a more sophisticated model), you decrease bias but increase variance. Conversely, simplifying the model decreases variance but increases bias.
- **Goal**: The goal is to find a balance where both bias and variance are minimized, allowing the model to generalize well to new, unseen data.

In summary:
- **High Bias** → Underfitting: The model is too simple.
- **High Variance** → Overfitting: The model is too complex.
- **Ideal Model** → Balanced Bias and Variance: The model generalizes well to new data.

| Problem           | Solution                                         |
| ----------------- | ------------------------------------------------ |
| high **variance** | Increase training examples                       |
| high **variance** | Using a smaller set of features                  |
| high **bias**     | Using additional features                        |
| high **bias**     | Using polynomial features (x₁², x₂², x₁x₂, etc.) |
| high **bias**     | Decreasing regularization parameter λ            |
| high **variance** | Increasing regularization parameter λ            |
![[Pasted image 20240815193214.png]]

### Why CNN over Feed Forward Neural Networks 
- An image is nothing but a matrix of pixel values, right? So why not just flatten the image (e.g. 3x3 image matrix into a 9x1 vector) and feed it to a MultiLevel Perceptron for classification purposes? Uh.. not really. 
- In cases of extremely basic binary images, the method might show an average precision score while performing prediction of classes but would have little to no accuracy when it comes to complex images having pixel dependencies throughout.
- **A ConvNet is able to successfully capture the Spatial and Temporal dependencies in an image through the application of relevant filters.** 
- The architecture performs a better fitting to the image dataset due to the reduction in the number of parameters involved and the reusability o weights. In other words, the network can be trained to understand the sophistication of the image better
##### Why do we need convolution operation 
- **Parameter sharing:** A kernel is shared among every section of the input. For example, an edge detector is useful in detecting edges at any part of the image, with just few numbers. 
- **Sparsity of connections:** each element of the output depends only on the small section of the input