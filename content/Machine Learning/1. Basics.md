# Machine Learning
### Cost Function

- Quantifies the difference between the predicted outputs of the model and the actual target values (the ground truth). 
- The primary goal in machine learning is to minimize this cost function, which will improve the model's accuracy.
- **If the cost function is not convex, it may have multiple local minima, which can make it difficult to find the global minimum.**

- **For regression problems:**   $J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right)^2$
  Here, $\hat{y}^{(i)}$ is the predicted value, $y^{(i)}$ is the actual value, m is the number of data points, and $\theta$ represents the parameters of the model.

- **For classification problems:**  $J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]$
- If it’s not spam, we want the model to give a low probability close to 0.
- If our model predicts a probability of 0.9 (which is close to 1) for a spam email, log⁡(0.9)\log(0.9)log(0.9) is a small negative number. But if it predicts 0.1, log⁡(0.1)\log(0.1)log(0.1) is a much larger negative number. Thus the large negative number result is what we were desiring
- The negative sign in the Cross-Entropy Loss flips these negative numbers to positive ones. This is because we want to **minimize** the loss, so lower numbers mean better predictions.

### Gradient Descent Algorithm

**Gradient Descent** is an optimization algorithm used to minimize the cost function by iteratively adjusting the parameters of the model. The algorithm takes steps proportional to the negative of the gradient (or approximate derivative) of the cost function with respect to the parameters.

1. **Initialize Parameters:** Start with some initial values for the parameters \( \theta \) (this could be random or zero).
  
2. **Compute the Gradient:** Calculate the gradient of the cost function with respect to each parameter. The gradient is a vector of partial derivatives, indicating the direction and rate of change of the cost function for each parameter.
   Gradient =  $\frac{\partial J(\theta)}{\partial \theta_j}$, For each parameter $\theta_j$ 
   
3. **Update Parameters:** Adjust the parameters in the opposite direction of the gradient to reduce the cost function. The size of the step is determined by the learning rate \( \alpha \).
   $\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}$

4. **Repeat:** Repeat the process until the cost function converges to a minimum (i.e., the changes in the cost function become negligibly small or a predefined number of iterations is reached).

#### Types of Gradient Descent:

- **Batch Gradient Descent:** Uses the entire dataset to compute the gradient at each step. This is computationally expensive for large datasets.

- **Stochastic Gradient Descent (SGD):** Uses only one training example to compute the gradient at each step. This is faster but introduces more noise into the process.

- **Mini-Batch Gradient Descent:** A compromise between batch and stochastic gradient descent, it uses a small batch of training examples to compute the gradient at each step.

### Local minima in gradient descent

1. **Stochastic Gradient Descent (SGD):** This introduces noise into the optimization process, which can help the algorithm escape local minima.
   
2. **Momentum:** helps accelerate gradient descent by adding a fraction of the previous update to the current update. This helps the algorithm maintain a ***consistent direction***, reducing the chance of getting stuck in local minima. The update rule with momentum is:
   
3. **Learning Rate Annealing:** Gradually decreasing the learning rate over time can help the algorithm to converge more smoothly and potentially avoid settling in local minima. **A high learning rate at the beginning allows the algorithm to explore more, while a lower learning rate later on allows for fine-tuning.**

4. **Adaptive Learning Rate Methods:**
   **AdaGrad, RMSProp, and Adam** are popular optimization algorithms used to train machine learning models. They adjust the learning rate during training to improve convergence and performance.
	
	- **AdaGrad (Adaptive Gradient Algorithm)**: 
	  Adapts the **learning rate for each parameter** based on the historical gradient information. Parameters that frequently receive large gradients have their learning rates decreased, while parameters with smaller gradients have their learning rates increased. This makes AdaGrad well-suited for dealing with sparse data. 
		- Effective for sparse datasets where some features are infrequent.
		- Disadvantages: Learning rate can decrease too much, leading to slow convergence or stopping too early.
	- **RMSProp (Root Mean Square Propagation)**: 
	  Addresses the **diminishing learning rate** issue in AdaGrad. It does so by introducing a decaying average of past squared gradients, which allows the learning rate to decrease more slowly over time, maintaining a balance between the history of gradients and the current gradient.
		- Prevents the learning rate from decreasing too much, as in AdaGrad.
		- Does not account for momentum, which can accelerate convergence.
	
	- **Adam (Adaptive Moment Estimation)**: Adam combines the benefits of both AdaGrad and RMSProp by computing adaptive learning rates for each parameter while also incorporating momentum. It uses running averages of both the gradient (first moment) and the squared gradient (second moment), making it robust and efficient for a wide range of problems.
5. **Batch Normalization:** Batch normalization normalizes the input to each layer, which can reduce the chances of getting stuck in local minima by smoothing the loss landscape.
   
6. **Random Restarts:** Run the gradient descent multiple times with different initializations. This increases the chances of finding the global minimum since the starting point might influence whether the algorithm converges to a local or global minimum.
   
7. **Simulated Annealing:** A gradual decrease in a "temperature" parameter, which controls the **probability of accepting worse solutions** as the optimization progresses. Initially, the algorithm is more likely to accept worse solutions, allowing it to escape local minima. As the temperature decreases, the algorithm focuses more on fine-tuning around the current solution.
### Convexity
The conditions for convexity of a function are:
1. $f''(x) \geq 0$
2. $f(\lambda \mathbf{x} + (1-\lambda) \mathbf{y}) \leq \lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y})$
	
3. $f(\mathbf{y}) \geq f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y} - \mathbf{x}) \quad \text{for all } \mathbf{x}, \mathbf{y} \in \mathbb{R}^n$
	Here, \( $\nabla f(\mathbf{x})$ \) is the gradient of \( $f$ \) at \( $\mathbf{x}$ \).
	
4. If the function \( $f$ \) is twice differentiable, it is convex if and only if its Hessian matrix
	$H(f)(\mathbf{x}) \succeq 0 \quad \text{for all } \mathbf{x} \in \mathbb{R}^n$
	The Hessian $H(f)(\mathbf{x})$ is the matrix of second-order partial derivatives: $H(f)(\mathbf{x}) =$
	![[Pasted image 20240816021210.png]]

### Decision Trees
**XGBoost** (Extreme Gradient Boosting) is a powerful and efficient open-source implementation of the gradient boosting framework.

- **Gradient Boosting:**
   Gradient boosting is an ensemble learning technique where multiple weak learners (typically decision trees) are combined to form a stronger model. It works by sequentially adding trees, where each new tree corrects the errors made by the previous trees. The goal is to minimize the loss function, often using gradient descent techniques.

- **Decision Trees as Weak Learners:**
   In XGBoost, the weak learners are decision trees. However, unlike traditional decision trees, these are often shallow (i.e., they have a limited depth), making them "weak" learners that can be easily improved upon by subsequent trees.

- **Additive Learning:**
   XGBoost builds the model in stages by adding new trees that fit the residual errors of the existing model. The new tree is added to the model to improve its predictions. This process is repeated iteratively, with each tree focusing on the mistakes of the previous ensemble.

6. **Parallel and Distributed Computing:**
   XGBoost is designed for efficient computation, supporting parallel and distributed computing. This allows it to handle very large datasets and train models much faster than traditional gradient boosting methods.

7. **Support for Custom Loss Functions:**
   XGBoost allows the use of custom loss functions, making it highly flexible for various types of machine learning problems.

8. **Feature Importance:**
   XGBoost provides built-in methods to measure the importance of each feature in making predictions, which can be useful for feature selection and understanding the model.

### Feature Extraction
Feature extraction is a crucial step in machine learning that involves transforming raw data into a set of features that can be used by a model to make predictions. These features are the most relevant pieces of information that help the model understand the underlying patterns in the data. 

1. **Raw Data**: This is the initial, unprocessed data that you have. It could be anything from text, images, audio, or structured data like spreadsheets. Raw data often contains noise, irrelevant information, or is in a form that isn't directly usable by a machine learning model.

2. **Features**: Features are the individual measurable properties or characteristics of the data. For example, in a dataset containing information about houses, features might include the number of bedrooms, square footage, or neighborhood. In image data, features might be edges, textures, or colors.

3. **Feature Extraction**: This is the process of transforming the raw data into a set of features. The goal is to reduce the data's dimensionality while retaining the most important information. Feature extraction can involve:
	   - **Dimensionality Reduction**: Techniques like Principal Component Analysis (PCA) reduce the number of features while preserving as much variance in the data as possible.
	   - **Transformation**: Converting data into a different form. For example, in natural language processing (NLP), text data might be transformed into numerical vectors using techniques like TF-IDF or word embeddings.
	   - **Selection**: Choosing a subset of the available features that are most relevant for the task. This could involve statistical tests, correlation analysis, or domain knowledge.

4. **Why Feature Extraction Matters**: 
	   - **Efficiency**: Reduces the computational cost and time required to train models by focusing only on the most important data.
	   - **Accuracy**: Improves model performance by removing irrelevant or redundant information that could confuse the model.
	   - **Interpretability**: Helps in understanding what aspects of the data are driving the predictions, making models easier to interpret.

5. **Examples of Feature Extraction Techniques**
	- **Text Data**: Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), Word Embeddings (e.g., Word2Vec, GloVe).
	- **Image Data**: Histogram of Oriented Gradients (HOG), Scale-Invariant Feature Transform (SIFT), Convolutional Neural Networks (CNNs).
	- **Time Series Data**: Fourier Transform, Wavelet Transform, statistical features like mean, variance, and autocorrelation.

### Feature Scaling
Feature scaling is a crucial preprocessing step in many machine learning algorithms for several reasons:

1. **Improving Convergence Speed**: If features are on different scales, the gradient descent can oscillate inefficiently, leading to slow convergence. Scaling the features helps standardize the step sizes and speeds up the convergence.
2. **Equal Contribution to Distance-Based Algorithms**
   - **Distance Metrics**: Algorithms like K-Nearest Neighbors (KNN), support vector machines (SVMs), and clustering algorithms (e.g., K-Means) rely on distance metrics (e.g., Euclidean distance). If features are not scaled, those with larger scales can dominate the distance calculations, leading to biased results.
3. **Ensuring Stability in Training**
   - **Regularization**: Regularization techniques (like L1 and L2) penalize large coefficients in linear models to prevent overfitting. If features are not scaled, the regularization terms may unfairly penalize coefficients corresponding to features with smaller scales, leading to suboptimal models.

4. **Improving Interpretability and Comparison**
   - **Interpreting Model Coefficients**: In models like linear regression, the coefficients represent the importance of each feature. If features are not scaled, interpreting these coefficients can be misleading because the magnitude of the coefficient depends on the scale of the feature. Scaling makes it easier to compare and interpret the relative importance of features.

 5. **Enhancing Model Performance**
   - **Uniform Sensitivity**: Some models are sensitive to the scale of the input data. Scaling ensures that the model treats all features uniformly, leading to better and more reliable predictions.

6. **Common Scaling Techniques**
   - **Min-Max Scaling**: Transforms features to a fixed range, usually [0, 1].
   - **Standardization (Z-score normalization)**: Transforms features to have a mean of 0 and a standard deviation of 1.
   - **Robust Scaling**: Uses the median and interquartile range for scaling, making it less sensitive to outliers.
### Linear Regression vs Logistic Regression

1. **Type of Dependent Variable:**
   - **Linear Regression:** It is used when the dependent variable (the outcome you're predicting) is *continuous*, meaning it can take any value within a range. Examples include predicting house prices, temperatures, or any numerical value.
   - **Logistic Regression:** It is used when the dependent variable is *categorical*, specifically binary (having two possible outcomes like 0/1, True/False, Yes/No). Examples include predicting whether a student will pass or fail, whether a patient has a disease or not, etc.

 2. **Output Interpretation:**
   - **Linear Regression:** The output of linear regression is a continuous value, which means it can theoretically be any *number*, positive or negative. However, this doesn't make sense when dealing with binary outcomes. For example, predicting a probability of -0.2 or 1.5 would not be meaningful.
   - **Logistic Regression:** The output of logistic regression is a *probability*, constrained between 0 and 1. This makes it suitable for binary *classification* tasks, as it provides a clear interpretation as a probability.

 3. **Modeling Relationship:**
   - **Linear Regression:** It assumes a linear relationship between the independent variables (predictors) and the dependent variable. This assumption works well for continuous data but can be problematic for categorical outcomes.
   - **Logistic Regression:** It models the probability of the binary outcome as a function of the independent variables using the logistic function, which creates an S-shaped curve. This allows for a more realistic model of binary outcomes, especially when the relationship between predictors and the probability of the outcome is not linear.

 4. **Application Domain:**
   - **Linear Regression:** Commonly used in regression analysis, forecasting, and predicting trends where the outcome is a continuous variable.
   - **Logistic Regression:** Commonly used in classification problems, such as spam detection, medical diagnosis, credit scoring, etc.

### Regularization
Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function of a model. The most common types of regularization are **L1 regularization** and **L2 regularization**. Both techniques modify the cost function but in different ways, leading to different effects on the model.
 1. **L2 Regularization (Ridge Regression):**
	**Cost Function:** $J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 + \frac{\lambda}{2} \sum_{j=1}^{n} \theta_j^2$
	**Effect:**
	- **Penalty Term:** $\frac{\lambda}{2} \sum_{j=1}^{n} \theta_j^2$ is the L2 regularization term, which penalizes large values of the coefficients.
	- **Effect on Weights:** L2 regularization tends to shrink the weights (parameters) but not to zero. The penalty encourages the model to distribute the weights more evenly across features, leading to smaller but non-zero weights.
	- **Smoothness:** Since the L2 penalty is proportional to the square of the weights, it heavily penalizes large weights, making the model smoother and less prone to overfitting.

2. **L1 Regularization (Lasso Regression):**
	**Cost Function: $J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 + \lambda \sum_{j=1}^{n} |\theta_j|$
	
	**Effect:**
	- **Penalty Term:** \( $\lambda \sum_{j=1}^{n} |\theta_j|$ \) is the L1 regularization term, which penalizes the absolute value of the coefficients.
	- **Effect on Weights:** L1 regularization tends to drive some of the weights to exactly zero, effectively performing feature selection by excluding some features entirely. This sparsity effect is a key difference from L2 regularization.
	- **Feature Selection:** Because L1 regularization can shrink some coefficients to zero, it is particularly useful when we believe that many features are irrelevant or when we want a simpler model with fewer features.
